{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b194e3cc",
   "metadata": {},
   "source": [
    "1. Define a `tokenizer` and `tokenizer_porter` function to be used in the model training pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9602b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00438647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer porter from the NLTK Porter Stemning algorithm\n",
    "# PIP install\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71137f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0a169",
   "metadata": {},
   "source": [
    "2. Define a `stop_word` function to be used in the model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a12615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stop_word():\n",
    "    return stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d342c",
   "metadata": {},
   "source": [
    "3. Read the movie data and break them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943f1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "import pandas as pd\n",
    "\n",
    "def split_data(data_path):\n",
    "    data = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "    # Data splits\n",
    "    X_train = data.loc[:25000, 'review'].values\n",
    "    y_train = data.loc[:25000, 'sentiment'].values\n",
    "    X_test = data.loc[25000:, 'review'].values\n",
    "    y_test = data.loc[25000:, 'sentiment'].values\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2556835",
   "metadata": {},
   "source": [
    "4. Logistic Regression Model training pipeline, with `GridSearchCV` as hyperparameter search strategy, Bag of Words for word embedding, and `LIBLINEAR` solver as the classifier.\n",
    "\n",
    "The earlier defined `tokenizer` and `tokenizer_porter` are also used for words' tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fea8cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neccesary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def train_test_data(train_x, train_y, test_x, test_y):\n",
    "    # Initialize the Bag of Words Embeddings\n",
    "    tfidf = TfidfVectorizer(strip_accents = None, lowercase = False, preprocessor = None)\n",
    "\n",
    "    # Set the parameter grid for the GridSearchCV\n",
    "    small_param_grid = [\n",
    "            {\n",
    "                'vect__ngram_range': [(1, 1)],\n",
    "                'vect__stop_words': [None],\n",
    "                'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                'clf__penalty': ['l2'],\n",
    "                'clf__C': [1.0, 10.0]\n",
    "            },\n",
    "            {\n",
    "                'vect__ngram_range': [(1, 1)],\n",
    "                'vect__stop_words': [stop_word, None],\n",
    "                'vect__tokenizer': [tokenizer],\n",
    "                'vect__use_idf': [False],\n",
    "                'vect__norm': [None],\n",
    "                'clf__penalty': ['l2'],\n",
    "                'clf__C': [1.0, 10.0]\n",
    "            },\n",
    "            {\n",
    "                'vect__ngram_range': [(1, 1)],\n",
    "                'vect__stop_words': [stop_word, None],\n",
    "                'vect__tokenizer': [tokenizer],\n",
    "                'vect__use_idf': [True],\n",
    "                'vect__norm': [None],\n",
    "                'clf__penalty': ['l2', 'l1'],\n",
    "                'clf__C': [1.0, 10.0]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    # Initialize the Logistic Regression-Bag of Words model training pipeline\n",
    "    lr_tfidf = Pipeline([\n",
    "            ('vect', tfidf),\n",
    "            ('clf', LogisticRegression(solver='liblinear'))\n",
    "        ])\n",
    "\n",
    "    # Attach the Logistic Regression-Bag of Words model training pipeline to the Hyperparameter search grid\n",
    "    gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid, scoring = 'accuracy', cv = 10, verbose = 2, n_jobs = 1)\n",
    "\n",
    "    # Fit the Grid search Logistic Regression-Bag of Words model training pipeline with the training set\n",
    "    gs_lr_tfidf.fit(train_x, train_y)\n",
    "    \n",
    "    print(\"==============================================\")\n",
    "    print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "\n",
    "    print(\"==============================================\")\n",
    "    print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')\n",
    "\n",
    "    print(\"==============================================\")\n",
    "    clf = gs_lr_tfidf.best_estimator_\n",
    "    print(f'Test Accuracy: {clf.score(test_x, test_y):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6eada6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc108d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_pipeline(data_path, data_type):\n",
    "    \n",
    "    print(f'This is the pipeline for {data_type}')\n",
    "\n",
    "    # Call the split_data function\n",
    "    data_split = split_data(data_path)\n",
    "\n",
    "    # Index the data split to get the respective splits\n",
    "    X_train =  data_split[0]\n",
    "    y_train = data_split[1]\n",
    "    X_test = data_split[2]\n",
    "    y_test = data_split[3]\n",
    "\n",
    "    # Call the train test pipeline\n",
    "    train_test_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d61119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58a63723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the pipeline for Unprocessed Movie Data\n",
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   5.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   7.3s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   7.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   6.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.5min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.5min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time= 1.4min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  12.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  12.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  11.4s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.3s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.3s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.4s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.4s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  12.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=  14.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   8.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   9.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.3s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.3s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "60 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2079, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1338, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    analyze = self.build_analyzer()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 454, in build_analyzer\n",
      "    stop_words = self.get_stop_words()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 376, in get_stop_words\n",
      "    return _check_stop_list(self.stop_words)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 200, in _check_stop_list\n",
      "    return frozenset(stop)\n",
      "TypeError: 'function' object is not iterable\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.87496539 0.87600545 0.89044459 0.88804497        nan 0.88024486\n",
      "        nan 0.87696501        nan 0.88360488        nan 0.86804539\n",
      "        nan 0.87768509        nan 0.87160536]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001ECD94FB6D0>}\n",
      "==============================================\n",
      "CV Accuracy: 0.890\n",
      "==============================================\n",
      "Test Accuracy: 0.893\n"
     ]
    }
   ],
   "source": [
    "# Unprocessed\n",
    "unprocessed_data_path = r\"C:\\Users\\Admin\\Documents\\Intro-to-ML\\NLP\\sentiment-analysis\\movie-review-data\\movie_data.csv\"\n",
    "call_pipeline(unprocessed_data_path , \"Unprocessed Movie Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798725d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6091112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the pipeline for Preprocessed Movie Data\n",
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001ECD94FB760>; total time=   4.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.9s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.9s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=<function stop_word at 0x000001ECE79A28C0>, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001ECD94FB6D0>, vect__use_idf=True; total time=   0.0s\n",
      "==============================================\n",
      "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001ECD94FB6D0>}\n",
      "==============================================\n",
      "CV Accuracy: 0.506\n",
      "==============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "60 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2079, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1338, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    analyze = self.build_analyzer()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 454, in build_analyzer\n",
      "    stop_words = self.get_stop_words()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 376, in get_stop_words\n",
      "    return _check_stop_list(self.stop_words)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py\", line 200, in _check_stop_list\n",
      "    return frozenset(stop)\n",
      "TypeError: 'function' object is not iterable\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.50577985 0.50577985 0.50577985 0.50577985        nan 0.50577985\n",
      "        nan 0.50577985        nan 0.50577985        nan 0.50577985\n",
      "        nan 0.50577985        nan 0.50577985]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.505\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed\n",
    "preprocessed_data_path = r\"C:\\Users\\Admin\\Documents\\Intro-to-ML\\NLP\\sentiment-analysis\\movie-review-data\\preprocessed_movie_data.csv\"\n",
    "call_pipeline(preprocessed_data_path, \"Preprocessed Movie Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371a9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
