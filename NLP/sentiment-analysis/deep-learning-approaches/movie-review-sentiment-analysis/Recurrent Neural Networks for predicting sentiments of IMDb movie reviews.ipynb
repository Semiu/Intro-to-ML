{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab Development Environment."
      ],
      "metadata": {
        "id": "Hj-N_eAp0XfA"
      },
      "id": "Hj-N_eAp0XfA"
    },
    {
      "cell_type": "markdown",
      "id": "902144e2",
      "metadata": {
        "id": "902144e2"
      },
      "source": [
        "0. Install neccesary libraries "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchdata torchvision torchtext torchaudio fastai\n",
        "!pip install portalocker\n",
        "!pip install --pre torch torchdata -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
      ],
      "metadata": {
        "id": "Hqg_SCDPXtHo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "81255dcb-8c49-4ee3-f897-fa4e362e2fc5"
      },
      "id": "Hqg_SCDPXtHo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 1.13.1+cu116\n",
            "Uninstalling torch-1.13.1+cu116:\n",
            "  Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[33mWARNING: Skipping torchdata as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torchvision 0.14.1+cu116\n",
            "Uninstalling torchvision-0.14.1+cu116:\n",
            "  Successfully uninstalled torchvision-0.14.1+cu116\n",
            "Found existing installation: torchtext 0.14.1\n",
            "Uninstalling torchtext-0.14.1:\n",
            "  Successfully uninstalled torchtext-0.14.1\n",
            "Found existing installation: torchaudio 0.13.1+cu116\n",
            "Uninstalling torchaudio-0.13.1+cu116:\n",
            "  Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "Found existing installation: fastai 2.7.11\n",
            "Uninstalling fastai-2.7.11:\n",
            "  Successfully uninstalled fastai-2.7.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-2.1.0.dev20230326%2Bcpu-cp39-cp39-linux_x86_64.whl (196.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata\n",
            "  Downloading https://download.pytorch.org/whl/nightly/torchdata-0.7.0.dev20230326-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from torchdata) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchdata) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch, torchdata\n",
            "Successfully installed torch-2.1.0.dev20230326+cpu torchdata-0.7.0.dev20230326\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvcYlXuuHGxc",
        "outputId": "ed0dca90-30c4-45b2-e8ef-db047632cca5"
      },
      "id": "TvcYlXuuHGxc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.15.1-cp39-cp39-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext) (2.27.1)\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.6.0\n",
            "  Downloading torchdata-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchtext) (3.10.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchtext) (1.11.1)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchtext) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchtext) (3.1.2)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchtext) (3.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from torchdata==0.6.0->torchtext) (1.26.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext) (67.6.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext) (0.40.0)\n",
            "Collecting lit\n",
            "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext) (3.25.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0->torchtext) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0->torchtext) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93601 sha256=fe8b4f24b60b383b23442d3b5ba4938099908381242fd5954b8ac45c8356c8f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ee/80/1520ca86c3557f70e5504b802072f7fc3b0e2147f376b133ed\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0.dev20230326+cpu\n",
            "    Uninstalling torch-2.1.0.dev20230326+cpu:\n",
            "      Successfully uninstalled torch-2.1.0.dev20230326+cpu\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.0.dev20230326\n",
            "    Uninstalling torchdata-0.7.0.dev20230326:\n",
            "      Successfully uninstalled torchdata-0.7.0.dev20230326\n",
            "Successfully installed lit-16.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import the libraries"
      ],
      "metadata": {
        "id": "LnaROAbZzsxB"
      },
      "id": "LnaROAbZzsxB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "54ArXIhRDhnt"
      },
      "id": "54ArXIhRDhnt",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB"
      ],
      "metadata": {
        "id": "uduYNaX2G-Jz"
      },
      "id": "uduYNaX2G-Jz",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "329ec716",
      "metadata": {
        "id": "329ec716"
      },
      "source": [
        "2a. Create the datasets: split the training dataset into seperate training and validation partitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "train_dataset = IMDB(split = 'train')\n",
        "test_dataset = IMDB(split = 'test')\n",
        "torch.manual_seed(1)\n",
        "# The original train dataset is 25000. This is being split randomly to 20000 and 5000 for training and validation\n",
        "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])\n",
        "# Get the 25000 for testing into a definite length\n",
        "test_dataset, _ = random_split(list(test_dataset), [25000, 0])"
      ],
      "metadata": {
        "id": "IWtK756zDwtt"
      },
      "id": "IWtK756zDwtt",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bc82276a",
      "metadata": {
        "id": "bc82276a"
      },
      "source": [
        "2b. Find unique tokens - as a preparation for encoding data for NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a07e41ff",
      "metadata": {
        "id": "a07e41ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86defcf3-bcbf-4c81-bc3a-0c0baf02084f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab-size: 69033\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "token_counts = Counter()\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        "\n",
        "print('Vocab-size:', len(token_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bee230",
      "metadata": {
        "id": "05bee230"
      },
      "source": [
        "3. Encode each unique token into integers - map each unique word (cleansed, preprocessed and tokenized by the `tokenizer` function) to a unique integer using the `vocab` method of `torchtext`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "48436986",
      "metadata": {
        "id": "48436986"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "\n",
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse = True)\n",
        "\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "vocab = vocab(ordered_dict)\n",
        "vocab.insert_token(\"<pad>\", 0) # Placeholder - Padding for adjusting sequence length\n",
        "vocab.insert_token(\"<unk>\", 1) # Placehlder - Unknown words\n",
        "\n",
        "vocab.set_default_index(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4377a654",
      "metadata": {
        "id": "4377a654"
      },
      "outputs": [],
      "source": [
        "#a. Define the function for transformation\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Transform each text in the dataset - using the defined tokenizer function\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "# Convert label to 1 and 0\n",
        "label_pipeline = lambda x: 1. if x == 'pos' else 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2bd8204b",
      "metadata": {
        "id": "2bd8204b"
      },
      "outputs": [],
      "source": [
        "#b. Wrap the encode and transformation function into this function\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype = torch.int64)\n",
        "        \n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "        \n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    # automatically pad consecutive elements for all sequences to have the same shape\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first = True)\n",
        "    \n",
        "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a small batch - to illustrate how padding works\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(train_dataset, batch_size = 4, shuffle = False, collate_fn = collate_batch)\n",
        "text_batch, label_batch, length_batch = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "3xHAvAADbvqM"
      },
      "id": "3xHAvAADbvqM",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c612028f",
      "metadata": {
        "id": "c612028f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d054428-8b5b-46db-da34-5ba4610311b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
            "             4,    18,    44,     2,  1705,  2459,   186,    25,     7,    24,\n",
            "           100,  1874,  1739,    25,     7, 34417,  3568,  1103,  7517,   787,\n",
            "             5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
            "         11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
            "          1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
            "         42948,     9,  4991,     3,    14, 10295,    34,  3568,     8,    51,\n",
            "           148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
            "          1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
            "         15994,   459,    34,  2479, 15211,  3713,     2,   840,  3200,     9,\n",
            "          3568,    13,   107,     9,   175,    94,    25,    51, 10296,  1796,\n",
            "            27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
            "           395,     2,   787,    32,    27,  5236,     3,    32,    27,  7251,\n",
            "          5118,  2460,  6389,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
            "           155,     3,  7014,     7,   409,     9,    41,   220,    17,    41,\n",
            "           390,     3,  3925,   807,    37,    74,  2857,    15, 10296,   115,\n",
            "            31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  216,   175,   724,     5,    11,    18,    10,   226,   110,    14,\n",
            "           182,    78,     8,    13,    24,   182,    78,     8,    13,   166,\n",
            "           182,    50,   150,    24,    85,     2,  4031,  5935,   107,    96,\n",
            "            28,  1867,   602,    19,    52,   162,    21,  1698,     8,     6,\n",
            "          1181,   367,     2,   351,    10,   140,   419,     4,   333,     5,\n",
            "          6022,  7135,  5055,  1209, 10892,    32,   219,     9,     2,   405,\n",
            "          1413,    13,  4031,    13,  1099,     7,    85,    19,     2,    20,\n",
            "          1018,     4,    85,   565,    34,    24,   807,    55,     5,    68,\n",
            "           658,    10,   507,     8,     4,   668,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [   10,   121,    24,    28,    98,    74,   589,     9,   149,     2,\n",
            "          7372,  3030, 14543,  1012,   520,     2,   985,  2326,     5, 16847,\n",
            "          5479,    19,    25,    67,    76,  3478,    38,     2,  7372,     3,\n",
            "            25,    67,    76,  2951,    34,    35, 10893,   155,   449, 29496,\n",
            "         23726,    10,    67,     2,   554,    12, 14543,    67,    91,     4,\n",
            "            50,    20,    19,     8,    67,    24,  4228,     2,  2142,    37,\n",
            "            33,  3478,    87,     3,  2563,   160,   155,    11,   634,   126,\n",
            "            24,   158,    72,   286,    13,   373,     2,  4804,    19,     2,\n",
            "          7372,  6793,     6,    30,   128,    73,    48,    10,   886,     8,\n",
            "            13,    24,     4,    85,    20,    19,     8,    13,    35,   218,\n",
            "             3,   428,   710,     2,   107,   936,     7,    54,    72,   223,\n",
            "             3,    10,    96,   122,     2,   103,    54,    72,    82,     2,\n",
            "           658,   202,     2,   106,   293,   103,     7,  1193,     3,  3031,\n",
            "           708,  5760,     3,  2918,  3991,   706,  3327,   349,   148,   286,\n",
            "            13,   139,     6,     2,  1501,   750,    29,  1407,    62,    65,\n",
            "          2611,    71,    40,    14,     4,   547,     9,    62,     8,  7943,\n",
            "            71,    14,     2,  5687,     5,  4868,  3111,     6,   205,     2,\n",
            "            18,    55,  2075,     3,   403,    12,  3111,   231,    45,     5,\n",
            "           271,     3,    68,  1400,     7,  9773,   932,    10,   102,     2,\n",
            "            20,   143,    28,    76,    55,  3810,     9,  2722,     5,    12,\n",
            "            10,   379,     2,  7372,    15,     4,    50,   710,     8,    13,\n",
            "            24,   887,    32,    31,    19,     8,    13,   428],\n",
            "        [18923,     7,     4,  4753,  1669,    12,  3019,     6,     4, 13906,\n",
            "           502,    40,    25,    77,  1588,     9,   115,     6, 21714,     2,\n",
            "            90,   305,   237,     9,   502,    33,    77,   376,     4, 16848,\n",
            "           847,    62,    77,   131,     9,     2,  1580,   338,     5, 18923,\n",
            "            32,     2,  1980,    49,   157,   306, 21714,    46,   981,     6,\n",
            "         10297,     2, 18924,   125,     9,   502,     3,   453,     4,  1852,\n",
            "           630,   407,  3407,    34,   277,    29,   242,     2, 20200,     5,\n",
            "         18923,    77,    95,    41,  1833,     6,  2105,    56,     3,   495,\n",
            "           214,   528,     2,  3479,     2,   112,     7,   181,  1813,     3,\n",
            "           597,     5,     2,   156,   294,     4,   543,   173,     9,  1562,\n",
            "           289, 10037,     5,     2,    20,    26,   841,  1392,    62,   130,\n",
            "           111,    72,   832,    26,   181, 12402,    15,    69,   183,     6,\n",
            "            66,    55,   936,     5,     2,    63,     8,     7,    43,     4,\n",
            "            78, 23727, 15995,    13,    20,    17,   800,     5,   392,    59,\n",
            "          3992,     3,   371,   103,  2595,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(text_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1dff40f7",
      "metadata": {
        "id": "1dff40f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f503e085-7599-4fd1-8ed7-691fddf71a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 0., 0.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(label_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ce00e78f",
      "metadata": {
        "id": "ce00e78f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e5df2b-a7fe-4594-e709-7907aec1bb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([165,  86, 218, 145], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(length_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "882e7fc0",
      "metadata": {
        "id": "882e7fc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5681c68f-f9f2-4228-f072-91ee30526156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 218])\n"
          ]
        }
      ],
      "source": [
        "print(text_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "169078ab",
      "metadata": {
        "id": "169078ab"
      },
      "outputs": [],
      "source": [
        "# Divide the datasets into data loaders with a batch size of 32 - to have it in a suitable format for RNN\n",
        "batch_size = 32\n",
        "train_dl = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, collate_fn = collate_batch)\n",
        "\n",
        "valid_dl = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, collate_fn = collate_batch)\n",
        "\n",
        "test_dl = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, collate_fn = collate_batch) #drop_last=True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf47fd09",
      "metadata": {
        "id": "bf47fd09"
      },
      "source": [
        "#### Embedding Layers for sentence encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "47262a0a",
      "metadata": {
        "id": "47262a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cd99f0-c101-4a31-92be-e4d651d16b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.3254, -0.9565, -1.8320],\n",
            "         [-0.0948, -0.9818,  0.3048],\n",
            "         [-0.5776, -0.2964,  0.2085],\n",
            "         [-0.5452, -2.8568,  1.1356]],\n",
            "\n",
            "        [[-0.5776, -0.2964,  0.2085],\n",
            "         [ 0.8829,  0.3068,  0.4394],\n",
            "         [-0.0948, -0.9818,  0.3048],\n",
            "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# To reduce the dimensionality of the word vector - dimensionality of the output\n",
        "embedding = nn.Embedding(num_embeddings = 10, embedding_dim = 3, padding_idx = 0)\n",
        "\n",
        "# a batch of 2 (rank of 2) samples of 4 indices each - input length of 4\n",
        "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n",
        "print(embedding(text_encoded_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea288cf7",
      "metadata": {
        "id": "ea288cf7"
      },
      "source": [
        "#### Building RNN for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2994d2c",
      "metadata": {
        "id": "c2994d2c"
      },
      "source": [
        "1. A recurrent layer of LSTM: for long sequences, LSTM layer supports long ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "eea379e8",
      "metadata": {
        "id": "eea379e8"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = 0) # Embedding layer\n",
        "        \n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first = True) # Recurrent layer of type LSTM\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size) # Fully connected layer as hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1) # Fully connected layer as an output layer\n",
        "        self.sigmoid = nn.Sigmoid() # Logistic sigmoid activation to produce single-class membership probability \n",
        "        \n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted = False, batch_first = True)\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 20 # feature size\n",
        "rnn_hidden_size = 64\n",
        "fc_hidden_size = 64\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba359b30",
      "metadata": {
        "id": "ba359b30"
      },
      "source": [
        "2. The `train` function to train the model on the given dataset for one epoch and return the classification accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "26f9e3ce",
      "metadata": {
        "id": "26f9e3ce"
      },
      "outputs": [],
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text_batch, lengths)[:, 0]\n",
        "        loss = loss_fn(pred, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "        total_loss  += loss.item()*label_batch.size(0)\n",
        "          \n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d6ee8b",
      "metadata": {
        "id": "58d6ee8b"
      },
      "source": [
        "3. The `evaluate` function to measure the model's performance on a given dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "984a8420",
      "metadata": {
        "id": "984a8420"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred = model(text_batch, lengths)[:, 0]\n",
        "            loss = loss_fn(pred, label_batch)\n",
        "            \n",
        "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "            total_loss  += loss.item()*label_batch.size(0)\n",
        "        \n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9163177e",
      "metadata": {
        "id": "9163177e"
      },
      "source": [
        "4. The `loss` function and `optimizer` (Adam Optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e27ca994",
      "metadata": {
        "id": "e27ca994"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss() # for binary classification and a single-class membership probability output\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d84fcce",
      "metadata": {
        "id": "0d84fcce"
      },
      "source": [
        "5. Train model for 10 epochs and display the training and validation performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3d6cc472",
      "metadata": {
        "id": "3d6cc472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc52e7c9-3c80-4a06-eade-5f7157a8c694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 1 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 2 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 3 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 4 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 5 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 6 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 7 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 8 accuracy:  1.0000 val_accuracy:  1.0000\n",
            "Epoch 9 accuracy:  1.0000 val_accuracy:  1.0000\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "torch.manual_seed(1)\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    print(f'Epoch {epoch} accuracy: {acc_train: .4f}' f' val_accuracy: {acc_valid: .4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2742c5d7",
      "metadata": {
        "id": "2742c5d7"
      },
      "source": [
        "6. Evaluate on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "65f1c5a8",
      "metadata": {
        "id": "65f1c5a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0acf143-3241-4d08-9f98-b38c10dcc8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy:  1.0000\n"
          ]
        }
      ],
      "source": [
        "acc_test, _ = evaluate(test_dl)\n",
        "print(f'test_accuracy: {acc_test: .4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yOfQCbc31du"
      },
      "id": "8yOfQCbc31du",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}