{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec and TextCNN"
      ],
      "metadata": {
        "id": "gpb-s5WWBopW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "eluxJL56nPRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your dataset)\n",
        "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\"]\n",
        "labels = [\"positive\", \"negative\", \"positive\"]\n",
        "\n",
        "# Tokenize and create Word2Vec embeddings\n",
        "tokenized_texts = [text.lower().split() for text in texts]\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "\n",
        "# Convert texts to sequences of indices\n",
        "X = torch.tensor([word2vec_model.wv[word] for sentence in tokenized_texts for word in sentence], dtype=torch.float32)\n",
        "X = X.view(len(tokenized_texts), -1, embedding_dim)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fHR9Ij5dsjo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TextCNN - Fully connected layer"
      ],
      "metadata": {
        "id": "PKiXQwAFs77L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the TextCNN model\n",
        "class TextCNN(nn.Module): # Use PyTorch module to declare the class\n",
        "    def __init__(self, embedding_dim, num_classes): # The features of the class's object - embedding dimension (how much can be understood) and the number of classes\n",
        "        super(TextCNN, self).__init__()  # Get the inherited power from  PyTorch's TextCNN\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3) # Defining the convolution layer for understanding the embedding dim by focussing on the kernel size at once, using 128 strength\n",
        "        self.fc1 = nn.Linear(128, num_classes) # making decision on the number of classes based on the 128 strength - fully connected layer\n",
        "\n",
        "    def forward(self, x): # Forwarding feeding training approch for class identification - input is the x (class labels)\n",
        "        # x = [batch size, sequence length, embedding dim]\n",
        "        x = self.conv1(x.permute(0, 2, 1)) # Convolution layer of the TextCNN model - the feature extractor\n",
        "        # x.permute changes the order of dimensions in the input tensor x - usually (batch_size, sequence_length, embedding_dim)\n",
        "        # It prepares the input for the convolution operation which takes the parameters (batch_size, sequence_length, embedding_dim)\n",
        "        # The conv1 is an instance of the convolution layer. It takes the permutated - i.e. transformed input, capturng local pattern from the embedding, kernel size (size of filter), number of filters\n",
        "\n",
        "        x = nn.functional.relu(x) # ReLu as activation function classifier algorithm\n",
        "        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2) # Max pooling - feature selection for best prediction, reducing the dimensionality of the features. The most important features\n",
        "        x = self.fc1(x) # The fully connected layers - high level features extracted from the text by the convolutional layer\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "8V8F9wvOmYpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Instantiation and Training"
      ],
      "metadata": {
        "id": "tVXBNXZOwoMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, loss function, and optimizer\n",
        "model = TextCNN(embedding_dim, num_classes=len(label_encoder.classes_)) # An instance of the TextCNN model\n",
        "criterion = nn.CrossEntropyLoss() # Loss function  - cross entropy is used, which measures the difference between the predicted class probabilities and the true class labels\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Optimizer to update the model parameters (weights and biases) during training. The lr is the learning rate\n",
        "\n",
        "# Training loop\n",
        "epochs = 5 # The number of times the model will go through the entire dataset\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Put the model in training mode\n",
        "    optimizer.zero_grad() # The optimizer gradient starts from zero\n",
        "    outputs = model(X_train) # Forward pass - computes the predicted output given the input\n",
        "    loss = criterion(outputs, y_train) # Calculated outputs arew compared to the true labels using the specified loss function\n",
        "    loss.backward() # Backpropagation - computes the gradients of the model parameters with respect to the loss. These are neccesary for adjusting the parameters\n",
        "    optimizer.step() # Adjust the model parameters\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\") # The epoch and the loss obtained - the lesser, the better\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = torch.sum(predicted == y_test).item() / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "tglfbVtRwops"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextCNN with only convolution layer"
      ],
      "metadata": {
        "id": "9edhv2bRnuDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the TextCNN model for only the convolution layer\n",
        "class TextCNNOnly(nn.Module): # Use PyTorch module to declare the class\n",
        "    def __init__(self, embedding_dim, num_classes): # The features of the class's object - embedding dimension (how much can be understood) and the number of classes\n",
        "        super(TextCNNOnly, self).__init__()  # Get the inherited power from  PyTorch\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3) # Defining the convolution layer for understanding the embedding dim by focussing on the kernel size at once, using 128 strength\n",
        "\n",
        "    def forward(self, x): # Forwarding feeding training approch for class identification - input is the x (class labels)\n",
        "        # x = [batch size, sequence length, embedding dim]\n",
        "        x = self.conv1(x.permute(0, 2, 1)) # Convolution layer of the TextCNN model - the feature extractor\n",
        "        # x.permute changes the order of dimensions in the input tensor x - usually (batch_size, sequence_length, embedding_dim)\n",
        "        # It prepares the input for the convolution operation which takes the parameters (batch_size, sequence_length, embedding_dim)\n",
        "        # The conv1 is an instance of the convolution layer. It takes the permutated - i.e. transformed input, capturng local pattern from the embedding, kernel size (size of filter), number of filters\n",
        "\n",
        "        x = nn.functional.relu(x) # ReLu as activation function classifier algorithm\n",
        "        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2) # Max pooling - feature selection for best prediction, reducing the dimensionality of the features. The most important features\n",
        "        return x"
      ],
      "metadata": {
        "id": "j8VJ9QL9oUXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec, High level features extracted from TextCNN convolution layer passed into ExtraTree classifier"
      ],
      "metadata": {
        "id": "qu_Rb5ZEtCZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract high-level features from TextCNN (with only convolution layer) for training and testing\n",
        "text_cnn_model = TextCNNOnly(embedding_dim, num_classes=len(label_encoder.classes_))\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_cnn_model.eval()\n",
        "    X_train_features = text_cnn_model(X_train).numpy()\n",
        "    X_test_features = text_cnn_model(X_test).numpy()\n",
        "\n",
        "# Use Extra Trees Classifier (you might want to fine-tune this based on your data)\n",
        "extra_trees_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train Extra Trees Classifier using high-level features\n",
        "extra_trees_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate the Extra Trees model using high-level features\n",
        "extra_trees_pred = extra_trees_model.predict(X_test_features)\n",
        "accuracy = accuracy_score(y_test, extra_trees_pred)\n",
        "print(f\"Extra Trees Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "vOMeb1P0nucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec embedding and ExtraTree classiifer"
      ],
      "metadata": {
        "id": "6V-e01HSbtdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Extra Trees Classifier (you might want to fine-tune this based on your data)\n",
        "extra_trees_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "extra_trees_model.fit(X_train.squeeze().numpy(), y_train)\n",
        "\n",
        "# Evaluate the Extra Trees model\n",
        "extra_trees_pred = extra_trees_model.predict(X_test.squeeze().numpy())\n",
        "accuracy = accuracy_score(y_test, extra_trees_pred)\n",
        "print(f\"Extra Trees Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "tThFsH-RcDIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}