{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec and TextCNN"
      ],
      "metadata": {
        "id": "gpb-s5WWBopW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your dataset)\n",
        "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\"]\n",
        "labels = [\"positive\", \"negative\", \"positive\"]\n",
        "\n",
        "# Tokenize and create Word2Vec embeddings\n",
        "tokenized_texts = [text.lower().split() for text in texts]\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "\n",
        "# Convert texts to sequences of indices\n",
        "X = torch.tensor([word2vec_model.wv[word] for sentence in tokenized_texts for word in sentence], dtype=torch.float32)\n",
        "X = X.view(len(tokenized_texts), -1, embedding_dim)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the TextCNN model\n",
        "class TextCNN(nn.Module): # Use PyTorch module to declare the class\n",
        "    def __init__(self, embedding_dim, num_classes): # The features of the class's object - embedding dimension (how much can be understood) and the number of classes\n",
        "        super(TextCNN, self).__init__()  # Get the inherited power from  PyTorch's TextCNN\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3) # Defining the convolution layer Understanding the embedding dim by focussing on the kernel size at once, using 128 strength\n",
        "        self.fc1 = nn.Linear(128, num_classes) # making decision on the number of classes based on the 128 strength\n",
        "\n",
        "    def forward(self, x): # Forwarding feeding training approch for class identification - input is the x\n",
        "        x = self.conv1(x.permute(0, 2, 1)) # Con\n",
        "        x = nn.functional.relu(x) # ReLu classifier algorithm\n",
        "        x = nn.functional.max_pool1d(x, x.size(2)).squeeze(2) # Max pooling - feature selection for best prediction\n",
        "        x = self.fc1(x) # Predict the class the input belong\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = TextCNN(embedding_dim, num_classes=len(label_encoder.classes_))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = torch.sum(predicted == y_test).item() / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "l3OcR9wDByLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}